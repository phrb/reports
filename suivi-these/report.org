# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer

#+TITLE: Autotuning under Tight Budget Constraints:
#+TITLE: @@latex: \\@@
#+TITLE: A Transparent Design of Experiments Approach
#+AUTHOR: Pedro Bruel, Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,titlepage]

#+LATEX_HEADER: \usepackage{pdfpages}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage[margin=2cm]{geometry}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \usepackage{sourcecodepro}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[scale=2]{ccicons}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepgfplotslibrary{dateplot}
#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   language=R,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \definecolor{Accent}{HTML}{157FFF}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\scriptsize\relax}
#+LATEX_HEADER: \graphicspath{{./img/}}

#+LATEX_HEADER: % https://tex.stackexchange.com/questions/129978/how-to-remove-section-subsection-titles
#+LATEX_HEADER: \newcommand{\fakesection}[1]{%
#+LATEX_HEADER:   \par\refstepcounter{section}% Increase section counter
#+LATEX_HEADER:   \sectionmark{#1}% Add section mark (header)
#+LATEX_HEADER:   \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}% Add section to ToC
#+LATEX_HEADER:   % Add more content here, if needed.
#+LATEX_HEADER: }
#+LATEX_HEADER: \newcommand{\fakesubsection}[1]{%
#+LATEX_HEADER:   \par\refstepcounter{subsection}% Increase subsection counter
#+LATEX_HEADER:   \subsectionmark{#1}% Add subsection mark (header)
#+LATEX_HEADER:   \addcontentsline{toc}{subsection}{\protect\numberline{\thesubsection}#1}% Add subsection to ToC
#+LATEX_HEADER:   % Add more content here, if needed.
#+LATEX_HEADER: }

#+LATEX: \clearpage
* Introduction
The use of heterogeneous programming models and computer architectures
in  High-Performance   Computing  (HPC)  has  increased   despite  the
difficulty  of   optimizing  and  configuring  legacy   code,  and  of
developing new solutions for heterogeneous computing. Due to the great
diversity of programming models and  architectures, there is no single
optimization strategy that fits all problems and architectures.

An optimization solution tailored for a specific problem requires time
and expert knowledge.   Using general optimizations can  be faster and
cheaper,  often  at  the   cost  of  application-specific  performance
improvements. A possible solution to  this trade-off is the automation
of the program optimization process.

The automated  selection of algorithms and  configuration of programs,
or  /autotuning/, casts  the program  optimization problem  as a  search
problem. The  possible configurations  and optimizations of  a program
are  used to  compose  a  /search space/,  and  search  is performed  by
evaluating the  impact of each  configuration on the  initial program.
Using the increasingly available  computing power to measure different
versions  of a  program, an  /autotuner/ searches  for good  selections,
configurations, and  optimizations. Each measurement provides  a value
for a meaningful program metric, such  as execution time and memory or
power consumption.

From the  implementation in a  high-level programming language  to the
instruction selection  in code  generation, it  is possible  to expose
optimization  and configuration  opportunities  in  various stages  of
program development and execution.  The time to measure the results of
an optimization choice depends on the selected stage and on the metric
to be  optimized.  Certain stages  are more  costly to measure  and to
optimize, because  the access  to special hardware  can be  costly and
constrained, and measurements may take a long time.

Search algorithms based on machine  learning or heuristics are not the
best candidates for autotuning  domains where measurements are lengthy
and costly, such as  compiling industrial-level FPGA programs, because
these  algorithms  rely on  the  availability  of  a large  number  of
measurements. They also assume good optimizations are reachable from a
starting position, and that tendencies  observed locally in the search
space  are exploitable.   These assumptions  are not  usually true  in
common  autotuning  domains,  as  shown  in the  work  of  Seymour  /et
al./\nbsp\cite{seymour2008comparison}.

Autotuning search spaces also  usually have non-linear constraints and
undefined   regions,  which   are  also   expected  to   decrease  the
effectiveness of search  based on heuristics and  machine learning. An
additional downside  to heuristics- and machine  learning-based search
is  that,  usually,  optimization  choices cannot  be  explained,  and
knowledge  gained  during  optimization  is not  reusable.   The  main
objective of this  thesis is to study how to  overcome the reliance on
these assumptions  about search  spaces, and  the lack  of explainable
optimizations, from the point of view of a Design of Experiments (DoE)
methodology to autotuning.

One of the  first detailed descriptions and  mathematical treatment of
DoE was presented by Ronald Fisher\nbsp\cite{fisher1937design} in his 1937
book  /The Design  of  Experiments/, where  he  discussed principles  of
experimentation, latin  square sampling and factorial  designs.  Later
books    such    as    the    ones    from    Jain\nbsp\cite{bukh1992art},
Montgomery\nbsp\cite{montgomery2017design}        and        Box        /et
al./\nbsp\cite{box2005statistics}  present  comprehensive and  detailed
foundations.  Techniques  based on  DoE are /parsimonious/  because they
allow  decreasing the  number  of measurements  required to  determine
certain  relationships   between  parameters  and  metrics,   and  are
/transparent/ because each choice of parameter value can be justified by
the results of statistical tests.

In  DoE terminology,  a /design/  is a  plan for  executing a  series of
measurements,  or   /experiments/,  whose   objective  is   to  identify
relationships  between  /factors/  and  /responses/.   While  factors  and
responses can refer  to different concrete entities  in other domains,
in computer  experiments factors  can be configuration  parameters for
algorithms  and  compilers, for  example,  and  responses can  be  the
execution time or memory consumption of a program.

Designs  can  serve  diverse   purposes,  from  identifying  the  most
significant factors for performance, to fitting analytical performance
models  for   the  response.   The   field  of  DoE   encompasses  the
mathematical  formalization   of  the  construction   of  experimental
designs.   More practical  works in  the field  present algorithms  to
generate designs with different objectives and restrictions.

This document presents a mid-term  report of the work performed during
this thesis.  The  work started in 2015, advised  by Professor Alfredo
Goldman, in the Computer Science PhD  program of the University of SÃ£o
Paulo, Brazil, which lasts 5 years.  The student started its /cotutelle/
thesis  at  the University  of  Grenoble  Alpes  in 2017,  advised  by
Professor Arnaud  Legrand, and the  predicted date for the  defense is
May 2020. This document  is organized as follows.  Sections\nbsp[[Autotuning
with  Search Heuristics]]  and\nbsp[[A Design  of Experiments  Methodology for
Autotuning]] describe succinctly the work done applying heuristics-based
search  algorithms   on  compiler  parameters  for   GPUs  and  FPGAs,
respectively.  Section\nbsp[[Objectives]]  discusses  our current  pursuit  to
refine our methodology for autotuning and include the user more in the
optimization process.  Section\nbsp[[Schedule]]  presents a tentative schedule
for future work. Appendix\nbsp\ref{sec:CV}  presents the student's /CV/, and
Appendices\nbsp\ref{sec:CCPE},    \ref{sec:reconfig},    \ref{sec:CCGRID},
\ref{sec:dopt} and\nbsp\ref{sec:analyseCCGRID}  present extensive material
that has been published or already written.

* Autotuning with Search Heuristics
We  initially  studied in  this  thesis  the effectiveness  of  search
heuristics, such  as simulated annealing, on  autotuning problems. Our
first target autotuning domain was the set of parameters of a compiler
for  GPU programs.   The search  heuristics for  this case  study were
implemented  using the  OpenTuner framework\nbsp\cite{ansel2014opentuner},
and consisted  of an  ensemble of search  heuristics coordinated  by a
Multi-Armed Bandit  algorithm.  The  autotuner searched  for a  set of
compilation parameters  that optimized  17 heterogeneous  GPU kernels,
from  a  set of  approximately  $10^{23}$  possible combinations  of  all
parameters.  With  1.5h autotuning  runs we have  achieved up  to $4\times$
speedup   in   comparison   with  the   CUDA   compiler's   high-level
optimizations.   The compilation  and execution  times of  programs in
this autotuning domain are relatively fast, and were in the order of a
few  seconds to  a  minute.  Since  measurement  costs are  relatively
small, search heuristics  could find good optimizations  using as many
measurements  as  needed.  A  detailed  description  of this  work  is
available  in our  paper\nbsp\cite{bruel2017autotuning}  published in  the
/Concurrency and Computation: Practice and Experience/ journal, which is
reproduced in Appendix\nbsp\ref{sec:CCPE}.

The   next   case   study   was  developed   in   collaboration   with
/Hewlett-Packard  Enterprise/,  and  consisted   of  applying  the  same
heuristics-based   autotuning  approach   to   the  configuration   of
parameters involved  in the generation of  FPGA hardware specification
from  source code  in  the  C language,  a  process called  /High-Level
Synthesis/ (HLS).  The main difference  from our work with GPU compiler
parameters was  the time to  obtain the hardware  specification, which
could be in the order of hours for a single kernel.

In this more complex scenario, we achieved up to $2\times$ improvements for
different  hardware  metrics  using  conventional  search  algorithms.
These  results were  obtained in  a  simple HLS  benchmark, for  which
compilation times were  in the order of minutes. The  search space was
composed of  approximately $10^{123}$  possible configurations,  which is
much  larger  than  the  search   space  in  our  previous  work  with
GPUs. Search space size and the  larger measurement cost meant that we
did  not  expect  the  heuristics-based  approach  to  have  the  same
effectiveness  as  in the  GPU  compiler  case  study. This  work  was
published\nbsp\cite{bruel2017autotuninghls} at the 2017 /IEEE International
Conference on ReConFigurable Computing and FPGAs/, and is reproduced in
Appendix\nbsp\ref{sec:reconfig}.

Approaches   using  classical   machine   learning  and   optimization
techniques  would  not  scale  to  industrial-level  HLS,  where  each
compilation can take hours to  complete.  Search space properties also
increase the  complexity of the  problem, in particular  its structure
composed   of  binary,   factorial  and   continuous  variables   with
potentially complex  interactions.  Our results on  autotuning HLS for
FPGAs  corroborate the  conclusion  that the  empirical autotuning  of
expensive-to-evaluate  functions, such  as  those that  appear on  the
autotuning  of  HLS,  require  a  more  parsimonious  and  transparent
approach, that can potentially be  achieved using the DoE methodology.
The next section describes our work on applying the DoE methodology to
autotuning.

* A Design of Experiments Methodology for Autotuning
Our application of the DoE methodology requires support for factors of
different types and  numbers of levels, such as  binary flags, integer
and floating  point numerical  values and enumerations  of categorical
values.  We also need designs  that minimize the number of experiments
needed for identifying  the most relevant factors of  a problem, since
at this moment we are not interested in a precise analytical model.

The  design construction  techniques that  fit these  requirements are
limited.   Considering flexibility  of application  and effectiveness,
the  best  candidate we  have  found  so  far are  /D-Optimal/  designs.
Considering that  we are  going to  analyze the  response of  a design
using  a linear  model  and ANOVA,  the /D-Efficiency/  of  a design  is
inversely proportional to the /geometric mean/ of the /eigenvalues/ of the
design's  /covariance   matrix/.   A  D-Optimal  design   has  the  best
D-Efficiency, and our current approach  is based on D-Optimal designs.
Appendix\nbsp\ref{sec:dopt}  presents  more  detailed description  of  the
computation of the D-Optimality criterion.

We obtained promising results with our approach on the autotuning of a
Laplacian kernel for GPUs, where the entire search space was available
and the performance model was  known.  Our approach consistently found
optimizations within 1% of the global optimum, while using half of the
allotted budget. We then performed  a more comprehensive evaluation of
our        method        on         the        SPAPT        autotuning
benchmark\nbsp{}\cite{balaprakash2012spapt},   which  contains   a  set   of
parametrized HPC  kernels with large  and complex search  spaces. This
work  was published\nbsp\cite{bruel2019autotuning}  at  the /2019  IEEE/ACM
International Symposium in Cluster, Cloud,  and Grid Computing/, and is
reproduced in Appendix\nbsp\ref{sec:CCGRID}.
* Objectives
We are  currently working on  improving our approach by  improving the
designs produced at  each step and by leveraging  user interaction and
knowledge more effectively .  We would  like to improve the quality of
the  designs produced  by our  approach, measured  by the  predictions
derived from a performance model  obtained from a linear regression on
experimental data, and by the D-Optimality criterion. We are achieving
improvements in  relation to  the standard  approach, and  the ongoing
work in this direction is presented in Appendix\nbsp\ref{sec:dopt}.

We  are  also   analyzing  the  results  from  our   recent  paper  on
CCGRID\nbsp\cite{bruel2019autotuning},            reproduced            in
Appendix\nbsp\ref{sec:CCGRID}, which were obtained by an automated version
of our approach.  We hope that these analyses  will help understanding
the  impact that  direct user  supervision  could have  had on  factor
elimination  and design  construction.   Our objective  is to  perform
non-automated  experiments  using  SPAPT   kernels,  to  showcase  the
potential  of user-assisted  optimization.  The  ongoing work  on this
direction is presented in Appendix\nbsp\ref{sec:analyseCCGRID}.

It would also be valuable to this  thesis to revisit the case study on
FPGAs, where  we would explore  the large search spaces  of industrial
FPGA kernels, with strong constraints  on measurement time, since such
kernels can  take up  to several  hours to  compile. This  study would
provide an interesting scenario to test our DoE approach.
* Schedule
Table\nbsp[[tab:sched]] presents a tentative schedule for the months until May
2020, which is the predicted date  for the defense. The remaining time
is divided in 3  periods of 5, 5 and 4 months.   We intend to continue
studying  and improving  our  approach  to autotuning  in  the next  2
periods,  extending the  work  presented in  Appendixes\nbsp\ref{sec:dopt}
and\nbsp\ref{sec:analyseCCGRID}.  These activities  are represented by the
lines /Sampling for D-Optimal Designs/ and /User-Centered Optimization/ in
Table\nbsp[[tab:sched]]. We  expect that the  studies and improvements  of our
approach will allow extending the work done in our CCGRID paper, which
we intend  to submit to  a journal.  We would  also like to  apply our
approach  to the  FPGA domain  in  the form  of a  case study.   These
activities would take the last  2 periods. Finally, the thesis writing
will take place during the last period.

#+NAME: tab:sched
#+ATTR_LATEX: :booktabs t :align llll
#+CAPTION: Tentative schedule for research activities
|------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| @@latex: \multirow{2}{*}{\textbf{Planned Research Activities}}@@ | @@latex: \multicolumn{3}{c}{\textbf{Periods}}@@                                                                                                             |
|                                                                  | @@latex: \multicolumn{1}{c}{\footnotesize{04/19-08/19}} & \multicolumn{1}{c}{\footnotesize{09/19-01/20}} & \multicolumn{1}{c}{\footnotesize{02/20-05/20}}@@ |
|------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| /Sampling for D-Optimal Designs/                                   | @@latex: \multicolumn{2}{c}{\cellcolor[HTML]{C0C0C0}} &@@                                                                                                   |
| /User-Centered Optimization/                                       | @@latex: \multicolumn{2}{c}{\cellcolor[HTML]{ACACAC}} &@@                                                                                                   |
| /Extended Paper/                                                   | @@latex: & \multicolumn{2}{c}{\cellcolor[HTML]{999999}}@@                                                                                                   |
| /Case Study on FPGAs/                                              | @@latex: & \multicolumn{2}{c}{\cellcolor[HTML]{868686}}@@                                                                                                   |
| /Thesis Writing/                                                   | @@latex: &  & \cellcolor[HTML]{737373}@@                                                                                                                    |
|------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|

#+begin_export latex
\bibliographystyle{plain}
\bibliography{references}
#+end_export

#+BEGIN_EXPORT latex
\clearpage
\appendix
\fakesection{CV}
\label{sec:CV}
\lhead{\textbf{A. CV}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/cv.pdf}

\fakesection{Publication at the CCPE Journal}
\label{sec:CCPE}
\lhead{\textbf{B. Publication at the CCPE Journal}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/ccpe16.pdf}

\fakesection{Publication at the IEEE ReConFig Conference}
\label{sec:reconfig}
\lhead{\textbf{C. Publication at the IEEE ReConFig Conference}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/reconfig17.pdf}

\fakesection{Publication at the CCGRID Conference}
\label{sec:CCGRID}
\lhead{\textbf{D. Publication at the CCGRID Conference}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/ccgrid19.pdf}

\fakesection{Comparing Sampling Strategies for Constructing D-Optimal Designs}
\label{sec:dopt}
\lhead{\textbf{E. Comparing Sampling Strategies for Constructing D-Optimal Designs}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/dopt_sampling.pdf}

\fakesection{Analysing Optimizations from the CCGRID Paper}
\label{sec:analyseCCGRID}
\lhead{\textbf{E. Analysing Optimizations from the CCGRID Paper}}
\includepdf[pages={1-},pagecommand={\thispagestyle{fancy}},frame=true,scale=.9]{pdf/analysing_ccgrid19.pdf}
#+END_EXPORT
